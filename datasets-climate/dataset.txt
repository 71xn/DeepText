On May 19, 2020, a group of engineers and emergency officials gathered at a fire station in Edenville, Michigan to decide what to do about the Edenville Dam, a 97-year-old hydroelectric structure about a mile upstream on the Tittabawassee River. Over the preceding two days, heavy rains had swelled the river, filling the reservoir to its brim and overwhelming the dam’s spillway. The group was just about to discuss next steps when the radios went off, recalled Roger Dufresne, Edenville Township’s longtime fire chief. “That’s when the dam broke.”
“Up at the dam, Edenville residents watched as a portion of the eastern embankment liquified. Muddy water gushed through the breach. Over the next few minutes, that water became a torrent, snapping trees and telephone poles as it rushed past town, nearly submerging entire houses further downstream.
About 10 miles and two hours later, the flood wave bowled into a second aging dam, damaging its spillway, overtopping, and then breaching the embankment.
Al Taylor, then chief of the hazardous waste section within the state’s Department of Environment, Great Lakes, and Energy, was following the situation closely as the surge swept 10 miles further downstream into the larger city of Midland, where it caused a Dow Chemical Company plant flanking the river to shut down, and threatened to mix with the plant’s containment ponds. Taylor, who retired at the end of January, worried that contamination from the ponds would spill into the river. But that was just the first of his concerns.
In prior decades, Dow had dumped dioxin-laden waste from the plant directly into the river, contaminating more than 50 miles of sediment downstream — through the Tittabawassee, the Saginaw River, and the Saginaw Bay — with carcinogenic material. The contamination was so severe that the U.S. Environmental Protection Agency stepped in, and since 2012, worked with Dow to map and cap the contaminated sediments. In designing the cleanup, engineers accounted for the river’s frequent flooding, Taylor knew, but nobody had planned for the specific impacts of flooding caused by a dam failure.
An Undark investigation has identified 81 other dams in 24 states, that, if they were to fail, could flood a major toxic waste site and potentially spread contaminated material into surrounding communities.While the dramatic breach of the Edenville Dam captured national headlines, an Undark investigation has identified 81 other dams in 24 states, that, if they were to fail, could flood a major toxic waste site and potentially spread contaminated material into surrounding communities.
In interviews with dam safety, environmental, and emergency officials, Undark also found that, as in Michigan, the risks these dams pose to toxic waste sites are largely unrecognized by any agency, leaving communities across the country vulnerable to the same kind of low-probability, but high-consequence disaster that played out in Midland.
After the flooding subsided, Dow and state officials inspected the chemical plant’s containment ponds and found that, though one of the brine ponds containing contaminated sediment had been breached, there was no evidence of significant toxic release. Preliminary sediment samples taken downstream did not find any new contamination. The plant’s and the cleanup’s engineering, it seemed, had done its job.
“Dow has well-developed, comprehensive emergency preparedness plans in place at our sites around the world,” Kyle Bandlow, Dow’s corporate media relations director, wrote in an email to Undark. “The breadth and depth of these plans — and our ability to quickly mobilize them — enabled the safety of our colleagues and our community during this historic flood event.”
But things could have gone differently — if not in Midland, then somewhere else with a toxic waste site downstream of an aging dam less prepared for a flood. “As a lesson learned from this,” Taylor said, “we need to be aware of that possibility.”
In the United States, there are more than 90,000 dams providing flood control, power generation, water supplies, and other critical services, according to the National Inventory of Dams database maintained by the U.S. Army Corps of Engineers, which includes both behemoths like the Hoover Dam and small dams holding back irrigation ponds. Structural and safety oversight of these dams falls under a loose and, critics say, inadequate patchwork of state and federal remit.
A 2019 report from the Congressional Research Service (CRS), the nonpartisan research arm of the U.S. Congress, found roughly 3 percent of the nation’s dams are federally owned, including some of the country’s largest, with the rest owned and operated by public utilities, state and local governments, and private owners. The report estimated that half of all dams were over 50 years old, including many that were built to now obsolete safety standards. About 15 percent of dams in the Army Corps database lacked data on when they were built.
In addition to information on age and design, the Army Corps database includes a “hazard potential” used to describe the possible impact of a dam failure to life and property. In 2019, roughly 17 percent, or 15,629 dams, had a high hazard potential rating, indicating that a loss of human life was likely in the event of a dam failure. The number of high-hazard dams has increased in recent years due to new downstream development.
According to the CRS report, more than 2,300 dams in the database were both high-hazard and in “poor” or “unsatisfactory” condition during their most recent inspection. Due to security concerns that arose after the September 11 terrorist attacks, the report did not name these dams, though an investigation by The Associated Press in 2019 identified nearly 1,700 of them.
For all that is known about America’s aging dam infrastructure, however, little information exists about the particular hazards dams pose to toxic waste sites downstream.For all that is known about America’s aging dam infrastructure, however, little information exists about the particular hazards dams pose to toxic waste sites downstream. This is why regulators knew about problems with the Edenville Dam and knew about the Dow cleanup, but had not connected the dots.
To identify dams that might pose the most serious risk to toxic waste sites, Undark searched for dams in the national database that are both high-hazard and older than 50 years, the age after which many dams require renovations. To narrow our search, we selected dams that sit 6 or fewer miles away from and appear in satellite images to be upstream of an EPA-listed toxic waste site. Experts say that many dams would flood much farther than 6 miles.
We then filed requests under state and federal freedom of information laws with various agencies, including the Federal Energy Regulatory Commission, seeking dam inspection reports and the Emergency Action Plans (EAPs) that dam owners are typically required to prepare and maintain. Among other things, these plans usually include inundation maps, which model the area that would likely be flooded in a dam failure scenario.
The inputs for these models vary by state, and while some inundation maps were highly sophisticated, involving contingencies for weather and other variables, others were less so. In one Emergency Action Plan for a dam in Tennessee, the inundation zone was simply hand-drawn on a map with a highlighter (see above image). But whatever their quality, the maps represent dam officials’ best estimate of where large volumes of water will flow if a dam fails.
Undark successfully obtained inundation modeling information for 153 of the 259 dams identified in our search. For 63 dams, state and federal officials declined to provide or otherwise redacted pertinent inundation information, citing security concerns. For 31 dams, agencies said they did not have inundation maps prepared, or provided maps that were illegible or did not extend to the site. Despite improvement in recent years, about 19 percent of high-hazard dams still lacked plans as of 2018, according to the American Society of Civil Engineers.
With those maps, we then looked to see if any EPA-listed toxic waste sites fell within the delineated inundation areas. Because the precise boundaries of each toxic waste site are not consistently available, we followed the methodology of a 2019 Government Accountability Office analysis of flood risks to contaminated sites on the EPA’s National Priorities List — more commonly known as Superfund sites — which used a 0.2-mile radius around the coordinates listed by the EPA for each location.
For a number of dams for which we were not able to obtain inundation maps to review ourselves, dam regulators or owners confirmed that coordinates we provided for the toxic waste site fell within 0.2 miles of the dam’s inundation zone.
We focused our search on the nation’s highest priority cleanup sites, as indicated by a designation of Superfund (for non-operating sites) or RCRA (Resource Conservation and Recovery Act of 1976, for operating sites). We considered 5,695 of these sites, including both current and former sites. Types and levels of contamination vary widely across sites, as would the impact of any flooding.
“There are many situations across the country then with these dams where they don’t meet the current safety standards.”Using this methodology, we identified at least 81 aging high-hazard dams that could flood portions of at least one toxic waste site if they failed, potentially spreading contaminated material into surrounding communities and exposing hundreds or thousands of people — in the case of very large dams, many more — to health hazards atop significant environmental impacts. At least six of the dams identified were in “poor” or “unsafe” condition during their most recent inspection.
In many instances, state and local agencies responsible for dam safety and toxic waste have not accounted for this kind of cascading disaster, and so remain largely unprepared.
Undark shared this analysis with engineering and dam safety experts, who verified the methodology. Several suggested that the true number of dams that could flood toxic waste sites if they were to fail is almost certainly far greater, but because no agency tracks this particular hazard, the actual number remains unknown.
“There are many situations across the country then with these dams where they don’t meet the current safety standards …” said Mark Ogden, a civil engineer who reviewed the American Society of Civil Engineer’s 2021 Infrastructure Report Card section on dams, which gave U.S. dams a “D” grade. “And the fact that there could be these hazardous sites as part of that, just increases that concern and what the consequences of a failure might be.”
Though impacts would vary widely, environmental scientists and toxicologists interviewed by Undark suggested that a sudden, intense flood caused by a dam failure could spread contaminants from hazardous waste sites into surrounding communities. Even sites designed to withstand flooding might be impacted if the debris carried in floodwater managed to scour and erode protective caps, potentially releasing toxic material into the water, explained Rick Rediske, a toxicologist at Grand Valley State University in Michigan. In Houston in 2017, flooding from Hurricane Harvey eroded a temporary protective cap at the San Jacinto River Waste Pits Superfund site, exposing dioxins and other toxic substances.
Water could then move contaminants around the site and redeposit them anywhere in the floodplain, exposing people and ecosystems to health hazards, said Jacob Carter, a research scientist at the Union of Concerned Scientists, who formerly studied flooding hazards to contaminated sites for the EPA. Carter also pointed out that communities living nearest to toxic waste sites and so most vulnerable to these events tend to be low income and communities of color.
It’s possible that any toxic material would be diluted by the flood and new clean sediment, said Allen Burton, director of the Institute for Global Change Biology at the University of Michigan. But this, he emphasized, would be a best-case scenario.
“You have no way of predicting, really, how much of the bad stuff moved, how far it moved, how far it got out into the floodplain, what the concentrations are.”Generally, when there’s a massive flood like the one in Michigan, “it just moves the sediment everywhere downstream,” said Burton. “You have no way of predicting, really, how much of the bad stuff moved, how far it moved, how far it got out into the floodplain, what the concentrations are.” And regulated waste sites are just one source of potential contamination in a dam breach scenario, said Burton. Sediment behind dams is itself often contaminated after years of collecting whatever went into the river upstream.
Contamination can also come from more mundane sources in the floodplain, like wastewater treatment plants or the oil canisters in people’s basements that get swept into floodwaters, said Burton. “The fish downstream,” he quipped, “don’t care if contaminants came from your garage or Dow Chemical.”
Undark’s investigation found that state and local governments often have not prepared for the flooding that could occur at toxic waste sites in the event of a dam failure.
Emporia Foundry Incorporated, a federally-regulated hazardous waste site in Greensville County, Virginia, provides a representative example. It falls within the inundation zone of the 113-year-old Emporia Dam, which is a hydroelectric dam partially owned by the city and located on the Meherrin River, just over one mile west of the foundry site.”
The foundry, which once manufactured manhole covers and drain grates, includes a landfill packed with byproducts containing lead, arsenic, and cadmium. The landfill was capped in 1984, and in 2014, a second cap was added nearer to the river as a buffer against flooding. As in Midland, cleanup engineers accounted for flooding within the 100-year floodplain, but according to a spokesperson from the Virginia Department of Environmental Quality, they did not account for flooding from a dam failure.
The Emporia Dam inundation map shows that if the dam were to fail during a severe storm, the entire foundry site could be flooded, potentially disintegrating the cap and spreading contaminants across the floodplain. However, the site would not be flooded in the event of a “sunny day” failure.
More than 3,000 people live within a mile of the Emporia Foundry site, around 75 percent of whom are Black, according to EPA and 2010 census data.
Wendy C. Howard Cooper, director of Virginia’s dam safety program, explained that her program’s mandate is to define a dam’s inundation zone and inform local emergency managers of any immediate risks to human life and property — not to identify toxic waste sites and analyze what might happen to them during a flood. “That would be a rabbit hole that no one could regulate,” Howard Cooper said. She added that local governments should be familiar with both the dams and the contaminated sites inside their borders and should have proper emergency procedures in place.
This turned out not to be true in Greensville County, where the program coordinator for emergency services, J. Reggie Owens, told Undark he was unaware of the potential for the foundry site to flood if the Emporia Dam were to fail. The site is “not even in the floodplain,” he said. “It’s never been put on my radar by DEQ or anyone else.”
A similar pattern emerged in other states. In Rhode Island, for instance, our search identified eight dams. One of these, the 138-year-old Forestdale Pond Dam, was considered “unsafe” during its most recent inspection.
Located in the town of North Smithfield, the dam is immediately adjacent to the Stamina Mills Superfund site, which once housed a textile mill that spilled the toxic solvent trichloroethylene into the soil. Another area on the site was used as a landfill for polycyclic aromatic hydrocarbons, sulfuric acid, soda ash, wool oil, plasticizers, and pesticides.
A few years after trichloroethylene was detected in groundwater in 1979, the site received a Superfund designation from the EPA. According to the federal agency, construction for the site cleanup — which involved removing the contaminated soil from the landfill and installing a groundwater treatment system — was completed in 2000 and accounted for a 100-year flood, but it did not account for flooding due to a dam failure.
According to EPA and census data, more than 2,500 people lived within a mile of Stamina Mills as of 2010, and Forestdale Pond is not the only dam that could pose a threat.
In fact, the site sits within the inundation zones of two other high hazard dams identified by Undark. A failure of either of these dams on the Slatersville Reservoir could cause a domino effect of dam failures downstream, according to Rhode Island dam safety reports, all leading to flooding at Stamina Mills.
When asked to comment on possible flood risks to the Superfund site, the EPA responded that the only remaining remedy at Stamina Mills, the groundwater treatment system, would not be affected if Forestdale Pond Dam were to fail. EPA made no reference to the larger Slatersville Reservoir dams less than two miles upstream.
Spokespersons at the Rhode Island dam safety office and the state office responsible for hazardous waste had not considered that a dam failure could flood any of the sites identified by Undark, including Stamina Mills.
By building engineered structures or taking other resiliency measures, the most hazardous waste sites can be designed to withstand flooding, explained Carter, who recently co-authored a report on climate change and coastal flooding hazards to Superfund sites. But in order to prepare for floods, Carter said, flooding hazards have to be recognized first, whether they come from rising seas, increasing storm surge, or, as in these cases, dams.
“They could have looked at that dam and said, ‘Oh, it gets a D minus for infrastructure. This thing could break.’”“They could have looked at that dam and said, ‘Oh, it gets a D minus for infrastructure. This thing could break,’” said Burton, referring to the Edenville Dam. “So in the future, it would be smart of EPA to require the principal party who’s responsible for the cleanup to look at the situation to see if it actually could happen.”
One step that could make that process much easier is for dam inundation zones to be regularly included in FEMA’s publicly available flood risk maps, which show the 100-year floodplain and other flood risks to communities, said Ogden. A lack of available data on dam inundations — sometimes the result of security concerns — presents a major obstacle, said a FEMA spokesperson, but plotting inundation zones on commonly-used flood risk maps would ensure communities and agencies are aware of and can respond to dam hazards.
Some states, including Rhode Island, have already made inundation zones, Emergency Action Plans, and inspection reports for the dams they regulate publicly available online. In South Carolina, following a 2015 event when heavy rains caused 50 dams to fail, dam inundations for the most hazardous state-regulated dams were made publicly available. Though no state agency tracks hazardous waste sites within dam inundation zones, Undark was able to identify three dams in South Carolina which could flood a hazardous waste site in the state using this resource.
In California, inundation zones for the state’s most hazardous dams were made available following a 2017 dam failure scare at the Oroville Dam, the tallest dam in the country, which led to the evacuation of more than 180,000 people.
Using this resource, Undark identified four dams which would flood at least one hazardous waste site in California. These included the Oroville Dam, which could flood at least one current and one former Superfund site if it were to fail.
According to the EPA, neither of those sites downstream of the Oroville Dam had considered the possibility of flooding due to dam failure prior to the failure scare. Even so, commented EPA, due to the “extraordinary volume of water” that would flood the sites if the Oroville Dam were to fail, “it is not feasible to alter the existing landfills and groundwater remedy infrastructure to protect against the potential failure of the Oroville Dam.”

In order to fix the nation’s dams, the first step is to spread awareness about the importance of dams and the hazards they pose to people and property.In order to fix the nation’s dams, the first step is to spread awareness about the importance of dams and the hazards they pose to people and property, said Farshid Vahedifard, a civil engineer at Mississippi State University who co-authored a recent letter in Science on the need to proactively address problematic dams. “The second thing is definitely we need to invest more.”
According to the Association of State Dam Safety Officials, the fixes necessary to rehabilitate all the nation’s dams would cost more than $64 billion; rehabilitating only the high hazard dams would cost around $22 billion. Meanwhile, the $10 million appropriated by Congress in 2020 for FEMA’s high hazard dam rehabilitation program are “kind of a drop in the bucket for what’s really needed,” said Ogden.
Indeed, state dam safety programs report a chronic lack of funds for dam safety projects, both from public sources and from private dam owners unable or unwilling to pay for expensive repairs. In Michigan, both dams that failed were operated by a company called Boyce Hydro, which received years of warnings from dam safety regulators that there were deficiencies.
Lee Mueller, Boyce Hydro’s co-manager, told Undark that the company made numerous improvements to the dams over the years. After losing revenue when the Federal Energy Regulatory Commission (FERC) revoked the company’s hydroelectric permit, however, it was unable to fund repairs that might have prevented the dam failures.
“Regarding the Edenville Dam breach, the subject of the State of Michigan’s governance and political policy failures and the insouciance of the environmental regulatory agencies are the subject of on-going litigation and will be more thoroughly detailed in the course of those legal proceedings,” Mueller wrote in an email.
“The state of Michigan knew about this,” said Dufresne, the Edenville fire chief. State regulators, he says, should have insisted that the company pay for the badly needed repairs. “They needed to push him,” said Dufresne, referring to Mueller. More than half of all dams in the U.S. are privately owned.
Without the funding to match the problem, members of the state dam safety community have looked to non-typical sources of funding, says Bill McCormick, chief of the Colorado dam safety program. In Eastern Oregon for example, the 90-year old Wallowa Lake Dam — which Undark found would flood the former Joseph Forest Products Superfund site if it were to fail — was slated last year for a $16 million renovation to repair its deteriorating spillway and add facilities for fish to pass through. But the plans have stalled since the Covid-19 pandemic has reduced Oregon’s lottery revenues, which were funding most of the project.
“If we start getting much bigger storms, then that itself will lead to a higher probability of overtopping and dam failure.”The challenges facing U.S. dams are also exacerbated by climate change, say dam safety experts, with more frequent extreme weather events and more intense flooding expected in parts of the country adding new stresses to old designs. “If we start getting much bigger storms, then that itself will lead to a higher probability of overtopping and dam failure,” said Upmanu Lall, director of the Columbia Water Center at Columbia University and co-author of a recent report on potential economic impacts of climate-induced dam failure, which considered how the presence of hazardous waste sites might further amplify damages. The report also outlines how in addition to more extreme weather, factors like changes in land use, sediment buildup, and changing frequencies of wet-dry and freeze-thaw cycles all can contribute to a higher probability of dam failure.
Several state dam safety programs contacted by Undark said they are planning for climate change-related impacts to dam infrastructure, though according McCormick, the Colorado dam safety chief, his state is the only one with dam safety rules which explicitly account for climate change. New rules that took effect in January require dam designs “to account for expected increases in temperature and associated increases in atmospheric moisture.”
“We were the first state to take that step, but I wouldn’t be surprised if others follow that lead,” McCormick said.
No deaths were reported in the Michigan flooding, but more than 10,000 residents had to be evacuated from their homes and the disaster likely caused more than $200 million in damage to surrounding property, according to a report from the office of Michigan Gov. Gretchen Whitmer. Restoring the empty reservoirs, as well as rebuilding the two dams, could cost upwards of $300 million, according to the Four Lakes Task Force, an organization that had been poised to buy the dams just before they failed.
In contrast, the Four Lakes Task Force, which now owns the dams, planned to spend about $35 million to acquire and repair those dams and an additional two dams prior to the breach. Boyce Hydro declared bankruptcy in July and now faces numerous lawsuits related to the flooding. FERC is coordinating with officials in Michigan on investigations into the dam failures, and has fined Boyce Hydro $15 million for failing to act on federal orders following the incident.
Dufresne, the Edenville fire chief, watched for years as political and financial challenges prevented the dams on the Tittabawassee from getting fixed. His advice for any other community dealing with a problematic dam: Call your state representatives, tell them, “Hey you need to investigate this.”
By August, life in Midland County was slowly getting back to normal. “Some of the people started putting their houses back together. The businesses are trying to figure out what to do next,” said Jerry Cole, the fire chief of Jerome Township, located south of Edenville.
At the Edenville Dam, neat houses looked out over a wide basin of sand-streaked mud where the impounded lake used to be. Near the bottom, where the river was still flowing through the gap in the fractured dam, a group of teenagers lounged on inner tubes, splashing around.

For more than five decades, satellites orbiting Earth have recorded and measured different characteristics of the land, oceans, cryosphere, and atmosphere, and how they are changing. Observations of planet Earth from space are a critical resource for science and society. With the planet under pressure from ever-expanding and increasingly intensive human activities combined with climate change, observations from space are increasingly relied upon to monitor and to inform adaptation and mitigation activities to maintain food security, biodiversity, water quality, and responsiveness to disasters.
A new cross-journal special collection, The Earth in Living Color, aims to provide a state-of-art and timely assessment of how advances in remote sensing is revealing new insights and understanding for monitoring our home planet.  We encourage papers that cover the use of imaging spectroscopy and thermal infrared remote sensing to observe and understand the Earth’s vegetation, coastal aquatic ecosystems, surface mineralogy, snow dynamics, and volcanic activity. These may range from architecture studies that determine spaceborne measurement objectives, to papers on algorithm development, calibration and validation, and modeling to support traceability. Papers can be submitted either to Journal of Geophysical Research: Biogeosciences or Earth and Space Science.
The special collection is associated with the NASA Surface Biology and Geology Designated Observable (SBG), and will document:
how SBG will meet science and applications measurement objectives;
how international partnerships (with the European Space Agency’s Copernicus Hyperspectral Imaging Mission (CHIME) and Land Surface Temperature Monitoring mission (LSTM) and with the Centre National d’Études Spatiales (CNES) and Indian Space Research Organization’s (ISRO) Thermal infraRed Imaging Satellite for High-resolution Natural resource Assessment mission (TRISHNA) will improve revisit times;
describe new developments in atmospheric correction, surface reflectance retrievals, and algorithms; and
detail synergies with other NASA Decadal Survey missions.
SBG leverages a rich heritage of airborne imaging spectroscopy that includes the AVIRIS and PRISM instruments, and thermal imagers such as HYTES and MASTER, as well space-based observations from pathfinder missions such as HYPERION, and current missions, including ECOSTRESS, PRISMA, DESIS, and HISUI.
Satellite measurements represent very large investments and the United States and space agencies around the globe organize their efforts to maximize the return on that investment. For instance, the US National Research Council conducts a decadal survey of NASA earth science and applications to prioritize observations of the atmosphere, ocean, land, and cryosphere. The most recent NASA Decadal survey, published in 2017, prioritized observations of surface biology and geology using a visible to shortwave infrared (VSWIR) imaging spectrometer and a multi-spectral thermal infrared (TIR) imager to meet a range of needs. As announced by NASA in May 2021, SBG will become integrated within a larger NASA Earth System Observatory (ESO)  that will include observations of aerosols, clouds, convection, and precipitation, mass change, and surface-deformation and change.
The SBG science, applications and technology build on over a decade of experience and planning for such a mission based on the previous Hyperspectral Infrared Imager (HyspIRI) mission study. During the course of a three-year study (2018-2021), the SBG team analyzed needed instrument characteristics (spatial, temporal and spectral resolution, measurement uncertainty) and assessed the cost, mass, power, volume, and risk of different architectures. The SBG Research and Applications team examined available algorithms, calibration and validation, and societal applications, and used end-to-end modeling to assess uncertainty.  The team also identified valuable opportunities for international collaboration to increase the frequency of revisit through data sharing, adding value for all partners. Analysis of the science, applications, architecture, and partnerships led to a clear measurement strategy and a well-defined observing system architecture.
SBG addresses global vegetation, aquatic, and geologic processes that quantify critical aspects of the land surface, responding to NASA’s Decadal Survey priorities, which then interact with the Earth’s climate system. The SBG observing system has a defined set of critical observables that equally inform science and environmental management and policy for a host of societal benefit areas. Click image for larger version. Credit: NASA JPL
First, and perhaps, foremost, SBG will be a premier integrated observatory for observing the emerging impacts of climate change. It will characterize the diversity of plant life by resolving chemical and physiological signatures. It will address wildfire, observing pre-fire risk, fire behavior and post-fire recovery. It will provide information for the coastal zone on phytoplankton abundance, water quality, and aquatic ecosystem classification. It will inform responses to natural and anthropogenic hazards and disasters guiding responds to a wide range of events, including oil spills, toxic minerals, harmful algal blooms, landslides and other geological hazards, including volcanic activity.
The NASA Earth System Observatory initiates a new era of scientific monitoring, with SBG providing an unprecedented perspective of the Earth surface through new spatial, temporal, and spectral information with high signal-to-noise. The Earth in Living Color special collection will showcase the latest advances in remote sensing that are providing vital insights into changes in planet Earth.
A jet stream known as the Chocó low-level jet (the ChocoJet) connects the Pacific Ocean with western Colombia. It helps dump more than 9,000 millimeters of rain each year, making the area offshore of the Colombian town of Nuquí one of the rainiest places on the planet.
“The ChocoJet—this low-level flow—is a physical bridge between the sea surface temperatures and sources of moisture in the Pacific, and the climate patterns of western South America.”“The ChocoJet—this low-level flow—is a physical bridge between the sea surface temperatures and sources of moisture in the Pacific, and the climate patterns of western South America,” said John Mejia, an associate research professor of climatology at Nevada’s Desert Research Institute and lead author of a new paper on the phenomenon.
In addition to its regional impact, the ChocoJet plays a role in the El Niño–Southern Oscillation (ENSO), a climate pattern whose variations can signal droughts and floods for Colombian farmers. ENSO also has significant impacts on Europe, Africa, Asia, and North America.
“In the atmosphere, we are all connected,” Mejia said, and the ChocoJet “is part of the engine that redistributes the heat from the tropics to higher latitudes.”
Rainy Puzzle
In 2019, after 6 months of preparations, Mejia and his team were able to get enough helium tanks and sonde balloons to this remote region (accessible by only sea or air). They launched the balloons up to four times a day over 51 days, resulting in new data on temperature, winds, and other atmospheric conditions.
“If you better understand the processes that are causing this high rainfall, you can find better ways for climate models to fill in the gaps where there [aren’t] hard data.”They detailed their findings in a recent paper published in the Journal of Geophysical Research: Atmospheres. The new data contribute to a better understanding of the dynamics and thermodynamics of the ChocoJet’s processes, which have implications for regional wildlife and agriculture, as well as for natural hazards. Mejia said the main contribution of the field campaign in Nuquí and the resulting data was to find out why and how these precipitation mechanisms produce one of the rainiest places on Earth, with the added benefit of building on the very scant climate data gathered previously. “This is a field experiment that can help test climate models.…Figuring this out can make global models more accurate,” Mejia said.
Alejandro Jaramillo, a hydrologist at the Center for Atmospheric Sciences at the National Autonomous University of Mexico, said that more observations will allow for a better model, which will lead to better prediction of rainfall and major weather events, like hurricanes. Jaramillo was not involved in the new research.
“If you better understand the processes that are causing this high rainfall, you can find better ways for climate models to fill in the gaps where there [aren’t] hard data,” Jaramillo said.
Impacts Beyond Climate
“With all the marvelous things I learned, my outlook changed completely, and my professional career changed course.”According to Germán Poveda, a coauthor on the recent study and a professor at the National University of Colombia, the project not only aimed to understand the dynamics and thermodynamics that explain the rainiest region on Earth but also was an opportunity to train Colombia’s next generation of climate scientists.
Juliana Valencia Betancur, for instance, was an undergraduate environmental engineering student at Colegio Mayor de Antioquia in Medellín during the Nuquí field campaign. She and a half dozen other undergraduate students were in Nuquí to help prepare and launch balloons as part of their research undergraduate experience.
Students in Nuquí, Chocó, Colombia, prepare to launch a sonde balloon during the fieldwork campaign.
Students from Institución Educativa Ecoturistica Litoral Pacifico in Nuquí prepare to launch a sonde balloon during the fieldwork campaign. Credit: Organization of Tropical East Pacific Convection (OTREC) participants/John Mejia
“I hadn’t had much interest in atmospheric science, but after Nuquí, with all the marvelous things I learned, my outlook changed completely, and my professional career changed course,” she said, adding that she is now looking for graduate opportunities in atmospheric science.
Johanna Yepes, a coauthor and researcher based at Colegio Mayor de Antioquia, said Nuquí’s local schoolchildren also benefited from the project’s outreach activities. During the field campaign, the researchers visited Nuquí’s only school and, with enthusiastic support of the principal, presented their project to students in the fourth to seventh grades. The students were also invited to visit the launch site, and two of them got a chance to launch a sonde balloon themselves.
“For me, it was the most beautiful part, putting what we were doing in very simple words and seeing how the children understood the daytime cycle of rain, sometimes even better than we did ourselves,” Yepes said.
Weather stations provide detailed records of temperature, precipitation, and storm events. These stations, however, are not always well spaced and can be scattered throughout cities or can even be absent in remote regions.
When direct measurements of weather are not available, researchers have a work-around. They use existing gridded climate data sets (GCDs) at different spatial resolutions that average weather within a specific grid. Unlike monitoring stations, the estimated temperatures in these grid cells are based on a combination of modeled forecasts and climate models as well as on observations (varying from ground monitors and aircraft to sea buoys and satellite imagery). These GCDs are very useful in large-scale climate studies and ecological research, especially in regions without monitoring stations.
But can GCDs be effective in epidemiological studies, for instance, in looking at how adverse temperatures might affect human health and mortality?
In a new study, de Schrijver et al. tested whether GCDs could be useful in studying temperature-related mortality in areas where weather stations are sparse. They compared gridded temperature data with weather station temperatures in two locations—England and Wales and Switzerland—to see whether one data set worked better than the other. These regions have varying topography, heterogeneous temperature ranges, and varying population distributions, all of which lead to pockets of irregular temperatures within an area.
To understand which temperature data would be most helpful in predicting health risks for communities, the researchers compared deaths from exposure to hot or cold temperatures for both GCDs and weather station data. They used weather station data from each country and a high- and low-resolution GCD (local and regional scales) to see which data were better for predicting risk of death from cold or heat.
The team found that both data sets predicted similar outcomes of health impacts from temperature exposure. However, in some cases, high-resolution GCDs were better able to capture extreme heat compared with weather station data when unequal distribution of the population was accounted for. This was especially the case in densely populated urban areas that experience notable temperature differences within them.
In the Cretaceous period, 100 million years ago give or take a few tens of millions, Earth was a very different place than today. Flowering plants and trees had only recently evolved to coexist with conifers, ferns, cycads, and other groups, while a diverse array of dinosaurs was the dominant form of megafauna on land. The global climate in which these plants and animals lived was also very different: warmer, steamier, and virtually devoid of ice.
Today, Earth is markedly cooler than the Cretaceous, and ice sheets and glaciers still cover large portions of the poles. Yet we know conditions are changing. The planet is already about 1°C warmer than it was during preindustrial times because of anthropogenic emissions of greenhouse gases like carbon dioxide (CO2), and many governments are working to limit further warming to no more than 2°C (or even 1.5°C) above preindustrial levels [Intergovernmental Panel on Climate Change (IPCC), 2018].
In this effort to project future climate scenarios, it is invaluable to investigate times in the geologic past when atmospheric carbon dioxide levels and temperatures were higher than today.Meanwhile, Earth scientists are critically evaluating the possibility and consequences of scenarios in which radiative forcing (the surplus in the amount of solar energy Earth absorbs compared to what it radiates back into space) drives temperatures beyond those targets. What will happen, they ask, if atmospheric CO2 levels (pCO2)—about 280 parts per million by volume (ppmv) in the preindustrial era and more than 415 ppmv now—reach 800–1,300 ppmv and the atmosphere warms as much as 5°C by 2100 [IPCC, 2018]? The ensuing climate change would raise sea levels and could produce drastic shifts in the hydrologic cycle that would exacerbate hazards like drought, floods, fire, and extreme temperatures—all of which could severely affect ecosystems and humans around the world.
In this effort to project future climate scenarios, it is invaluable to investigate times in the geologic past when pCO2 levels and temperatures were higher than today because these are the best natural analogues that we have to provide reference points for the future [Tierney et al., 2020]. One such time of interest is the Cretaceous (145.5 million to 66.0 million years ago), when atmospheric conditions created an intense “greenhouse” climate on the planet.
In 2006, workers on the SK project, an ambitious terrestrial paleoclimatic and paleoenvironmental research effort in China’s Songliao Basin, began drilling through the basin’s rocks to obtain the first complete record of the terrestrial Cretaceous climate (Figure 1). The project, named SK from the Chinese Pinyin of the phrase “Songliao Scientific Drilling,” is being conducted under the framework of the International Continental Scientific Drilling Program (ICDP) [Wang et al., 2013a, 2013b; Gao et al., 2019]. Findings following the first two phases of drilling have already revealed valuable insights. With the final phase of drilling just completed, we anticipate that new research will lead to additional revelations about past—and potential future—climate conditions.
Containers of sediment core line the floor of a long storage room
Fig. 1. Core segments from the SK drilling project in the Songliao Basin are stored in this facility in Beijing. Credit: Yuan Gao
A Gold Standard for Reading Cretaceous Climate
The Cretaceous period is an archetypal example of a greenhouse climate. Atmospheric pCO2 levels reached as high as about 2,000 ppmv, average temperatures were roughly 5°C–10°C higher than today, and sea levels were 50–100 meters higher [O’Brien et al., 2017; Tierney et al., 2020]. These conditions resemble the most extreme scenario that the IPCC has predicted could occur by the end of this century, with pCO2 levels greater than 1,200 ppmv and global temperatures roughly 4°C higher [IPCC, 2018].
The Cretaceous represents the last gasp of dinosaurs’ dominance in Earth’s ecosystem; in addition, it was a time of rapid evolutionary turnover and proliferation of mammals, birds, and angiosperms (flowering plants). For decades, the scientific community has been thoroughly engaged in understanding Cretaceous climate, especially how events such as the Chicxulub asteroid impact and Deccan volcanism contributed to evolution and to the extinction of nonavian dinosaurs and other biomes [Hull et al., 2020]. Despite this intense interest, we still lack long and continuous continental geological records of the Cretaceous.
The Songliao Basin preserves a continuous and complete terrestrial record of the Cretaceous.In northeastern China, the Songliao Basin sprawls over roughly 260,000 square kilometers and, buried amid layers of mudstone, siltstone, and other sedimentary rocks, holds rich petroleum resources that support the Daqing Oilfield, one of the largest oil fields in China and the world [Wang et al., 2013a, 2013b]. The Songliao Basin is also one of the largest continental sedimentary basins and, with a maximum depth of more than 10,000 meters, preserves a continuous and complete terrestrial record of the Cretaceous [Wang et al., 2013a].
The SK project includes three main scientific objectives. The first is to assemble, from drill cores collected, a terrestrial “gold pillar” for the Cretaceous—a new high-resolution standard for correlating and dating Cretaceous terrestrial and marine strata and for identifying key stratigraphic boundaries. Such boundaries include those marking the Jurassic-Cretaceous and Cretaceous-Paleogene (K-Pg) transitions, as well as terrestrial responses to Cretaceous oceanic anoxic events (OAEs) when oxygen levels in the ocean dropped precipitously.
The second objective is to study Cretaceous terrestrial climate change and its links to biological evolution and extinction in East Asia, which will help us understand the responses of northern midlatitude climate to greenhouse-driven change. The third objective is to understand the mechanisms and processes behind the massive accumulation of organic matter in this large, long-lived lake basin, which will enhance our knowledge of the petroleum resource in the Daqing Oilfield.
Three Phases and Four Holes
The drilling portion of the SK project included three phases (Figure 2). The first phase, SK-1, was completed in 2006–2007 with two boreholes drilled near Daqing City, Heilongjiang Province. This phase recovered 2,486 meters of core dating from the Late Cretaceous to early Paleogene that includes the 66-million-year-old K-Pg boundary representing the last mass extinction and the demise of dinosaurs [Wang et al., 2013a; Gao et al., 2015, 2019].
Cross sectional diagram of the Songliao Basin showing layers of sedimentary rock and the SK project’s drilling and coring strategy
Fig. 2. Cross section of the Songliao Basin from northwest to southeast, showing named layers of sedimentary rock in the basin as well as the drilling and coring strategy of the three phases of the SK project. Click image for larger version.
The second phase, SK-2, was completed in 2014–2018 in Anda County, Suihua City, Heilongjiang Province. SK-2 drilled to a depth of 7,018 meters, the deepest ever by an ICDP project at the time, and recovered 4,134 meters of core. This core stretches from Permian-Triassic basement rock (roughly 200 million to 300 million years old) to Early Cretaceous sediments [Gao et al., 2019].
In total, the three phases of the SK project have recovered about 8,200 meters’ worth of terrestrial sediments spanning the entire Cretaceous.Drilling for the third phase, SK-3, started in September 2020 in Nong’an County, Changchun City, Jilin Province, and as of late January, nearly 1,600 meters of terrestrial sediments from the middle Cretaceous have been recovered. This time period was the warmest interval during the past 150 million years and includes OAE2 about 94 million years ago. This event is considered one of the largest perturbations to Earth’s carbon cycle, a time when organic material piled up on the seafloor because of decreased oxygen levels in the ocean.
In February 2021, the SK project completed all drilling and coring. In total, the three phases have recovered about 8,200 meters’ worth of terrestrial sediments spanning the entire Cretaceous.
Deciphering the Story in the Strata
Research since 2006 investigating the SK-1 and SK-2 cores has resulted in many multidisciplinary scientific achievements. A precise chronostratigraphic framework—basically, a timeline of sedimentary layers—that pins down dates to within 100,000 years has been established for the SK-1 cores. A variety of methods were used to construct this timeline: radiometric dating (analyzing the decay rates of different radioactive isotopes), magnetostratigraphy (tracking orientations of magnetic polarity preserved in rock layers), biostratigraphy (comparing fossils of particular ages across different layers), and cyclostratigraphy (tracking orbitally induced climate change cycles recorded in the sediments) [Wu et al., 2014]. The resulting age model allows direct correlation of data from the Songliao Basin with data from sedimentary records collected elsewhere in the Cretaceous world.
Stable carbon and oxygen isotopes preserved in fossil ostracods, tiny lake-dwelling crustaceans, from the Songliao were found to record both an overall cooling trend through the Late Cretaceous and local signals of lake basin evolution [Chamberlain et al., 2013]. Meanwhile, studies of sedimentary structures, mineralogical compositions, and stable isotopes in lake and paleosol (ancient soil) sediments indicated that large fluctuations in air temperatures over land, humidity, and moisture sources occurred during the last 10 million years before the dinosaurs went extinct [Gao et al., 2015]. These findings have demonstrated that considerable climatic changes occurred before the demise of dinosaurs, and they have contributed to the ongoing debate over relationships between climate change, volcanism, the Chicxulub impact, and mass extinction [e.g., Hull et al., 2020].
Findings based on studies of the SK cores have also fueled questions about ancient seawater incursions into the paleo-Songliao lake, evidence for which comes from microfossils of marine foraminifera and other marine organic biomarkers found in intervals of the SK cores [Wang et al., 2013b; Xi et al., 2016]. Although the cores dominantly record an ancient lake environment during this time, episodic and fast incursions of seawater may have altered the chemistry of lake water and promoted the preservation of organic matter under anoxic (oxygen-poor) conditions.
Several widespread disturbances in Earth’s carbon cycle—represented by OAEs—are known to have occurred during the Cretaceous from records of marine organic carbon burial. However, until recently, no direct links had been found between these marine data and records of terrestrial organic carbon burial during the same time. Analysis of the early SK cores revealed geochemical markers of OAE3 about 88 million years ago, the last Cretaceous OAE, that are contemporaneous with markers from the Western Interior Seaway (the shallow sea that covered much of what is now North America), providing evidence that this carbon cycle disruption indeed affected both land and sea [Jones et al., 2018].
New Research on the Horizon
These studies will help us understand just how hot the terrestrial realm was during the warmest time period in the past 150 million years and how hot it may get in the future in the northern midlatitudes.Research on the new core section retrieved during the recently completed SK-3 drilling phase will explore questions about the evolution of terrestrial climate in East Asia during the middle Cretaceous, the warmest time period in the past 150 million years [O’Brien et al., 2017; Tierney et al., 2020]. These studies will help us understand just how hot the terrestrial realm was at that time and how hot it may get in the future in the northern midlatitudes, in which more than 40% of the global population lives today.
This drilling will also provide a new, terrestrial record of OAE2, which is characterized by widespread distribution of marine black shales in major ocean basins and represents a major disturbance of the global carbon cycle. This Cretaceous terrestrial record from the SK cores in the Songliao Basin will be further integrated with counterpart marine records from the Western Interior Seaway in North America to understand more fully land-ocean linkages at Earth’s surface, for example, whether lakes were anoxic, like the oceans [Wang et al., 2013b].
The SK project team invites the geoscience community to participate in collaborative research investigating the SK cores, which span continuously from the Late Jurassic to the early Paleogene. Following regulations established by oceanic scientific drilling programs, the SK cores are available in a long-term repository in Beijing and are completely open to Earth scientists from all over the world.
We anticipate that through the combined efforts of experts from a variety of research fields, the more than 8,000 meters of cores from the Songliao Basin will elucidate the history and mechanisms of interactions among the climate system, biosphere, and lithosphere in the age of dinosaurs during Earth’s most intense greenhouse state of the past 150 million years.
From 50,000 to 15,000 years ago, during the last ice age, Earth’s climate wobbled between cooler and warmer periods punctuated by occasional, dramatic ice-melting events.
Previous research has suggested that these oscillations were likely influenced by changes in the Atlantic Meridional Overturning Circulation (AMOC), a pattern of currents that carry warm, tropical water to the North Atlantic, where it cools, sinks, and flows back south. However, the precise role played by the AMOC in ancient climate fluctuations has been unclear.
Now Toucanne et al. have reconstructed the historical flow of a key current in the upper part (the northward flow) of the AMOC, the Glacial Eastern Boundary Current (GEBC), shedding new light on how the AMOC can drive sudden changes in climate.
The GEBC flowed northward along Europe’s continental margin during the last ice age (it persists today as the European Slope Current). To better understand the GEBC’s role in the AMOC, the researchers collected six seafloor sediment cores off the coast of France. Analysis of grain sizes and isotope levels in the core layers revealed the current’s strength when each layer was deposited, yielding the first high-resolution, 50,000-year historical record of the current.
This new historical record shows that the GEBC flowed faster during warmer intervals of the last ice age but weakened during the coldest periods. The timing of these changes aligns well with previously established records on AMOC speed and the southward return flow of deep waters to the west.
Comparing the history of the GEBC with other records also shows that major ice-melting events, in which ice age glaciers released huge amounts of freshwater into the Atlantic, correspond with periodic weakening of the current and of the AMOC in general.
Drawing on these findings, the researchers outline a mechanism by which the GEBC could have carried cold glacial meltwater northward and contributed to changes in the AMOC that may have driven warm-cold climate oscillations in the North Atlantic. Further research could help clarify these dynamics.

As social and economic activity ground to a halt around the world in 2020 in the face of the COVID-19 pandemic, emissions of greenhouse gases and aerosols dropped. In the United States, where lockdowns that began in March kept many Americans off the roads, emissions fell by almost 13%. Globally, carbon dioxide (CO2) emissions dropped by nearly 7%. However, it’s not clear that emissions will remain low as global economies open back up, which raises the question: What impact will the short-term drop in emissions have on the climate?
In a new study, Jones et al. answer that question by comparing the results from an ensemble of a dozen Earth system models. Such multi–Earth system model intercomparison projects (MIPs), which usually take years to design and carry out, can detect even small climate signals. The first results from the current project, known as COVID-MIP, focus on the immediate impacts of COVID-related emissions decreases and assume that global emissions will rebound to baseline levels by 2022.
The models showed a decrease in aerosol optical depth—a measure of how much sunlight is blocked from reaching Earth’s surface because of aerosol particles—and an increase in the amount of solar radiation reaching the planet’s surface, with the greatest impact seen over India and China. The authors then examine how the changes in aerosol levels might affect shortwave radiation, temperature, and precipitation patterns.
The researchers saw a slight increase in the amount of solar radiation reaching the planet’s surface, but there was little impact on either temperature or rainfall. The biggest aerosol reductions were over Asia, but even there, most models showed a small amount of warming, less than 0.1°C—about half the size of the standard deviation across the models in the ensemble.
The authors conclude that the drop in global emissions due to COVID-19 is too small in both magnitude and duration to have any significant impact on global climate. Still, the results can direct priorities for future work, and the authors identify seven areas where future analyses may be warranted, including the longer-term implications of the emissions reductions and of economic recovery decisions.
Northern Europe is not a place we think of as experiencing water limitation. In this normally damp and cloudy region, increases in air temperature or sunlight are correlated with higher evapotranspiration rates and increased humidity surface humidity. During the anomalously dry summer of 2018, however, Dirmeyer et al. [2021] show that soil moisture in Great Britain and large parts of Northern Europe dropped below a threshold where increasing air temperatures no longer correlated with surface humidity. Instead, these regions switched a water limited regime, where evapotranspiration could not cool the surface as efficiently. This further exacerbated extreme hot and dry conditions, a net positive feedback.
In the accompanying viewpoint, Orth [2021] highlights the importance of identifying such thresholds in coupling energy and water balance between soils, plants and the atmosphere, and how improved models can help predict the course of future, increasingly common, weather extremes.
Stalagmites are a chemical precipitate found in caves that form from drip waters and grow from the floor toward the ceiling. Various environmental and climatic factors influence how they grow and develop layers. A recent article in Reviews of Geophysics analyzed a global dataset of laminated stalagmites. Here, the lead author explains what influences laminated stalagmites in different locations and what the analysis revealed.
What are stalagmites and how do they grow?
Stalagmites commonly accumulate in a vertical direction and typically have an associated stalactite, which forms on the cave ceiling and grows downward.
Stalagmites are built by layers of calcite crystals.They are built by layers of calcite crystals, which may be perfectly stacked one on top of the other if nothing disturbs the growth; however, there are many disturbances in caves.
For example, if tiny particles (known as colloids) are transported from the soil onto the stalagmite, this disturbs the stacking, and may create pores between growing crystals or even slightly change their shape. The addition of certain trace elements also disturbs the growth because they may influence the morphology of the growing crystals, known as “fabric.”
Where there is a seasonal climate, these changes in fabric can occur seasonally, producing layers which we call “annually laminated stalagmites.” Annual layers can also be observed in the trace element, tiny particles, and organic material composition of stalagmites.
Does the presence of annual lamination seen in stalagmites vary from place to place? If so, what are the main influencing factors?
We analyzed a global database to find out about the geographic distribution of laminated stalagmites. We found that they occur in caves that developed within a wide range of host rock geological ages (from Ordovician to the present-day) and lithologies (limestone, dolomite, and aeolianites). Geographically, laminated stalagmites have been reported from between 35 °South and 66 °North. They have also been reported from a very wide range of mean annual temperature (from 1.4 to 26 °C) and total annual precipitation (from 74 to 2052 millimeters).
Locations of laminated stalagmites investigated in Baker et al. [2021].
Locations of laminated stalagmites investigated in Baker et al. [2021]. Grey shaded areas are the global karst extent (Goldscheider et al., 2020). Credit: Laia Comas-Bru
The main requirement for the presence of laminated stalagmites is the seasonality of rainfall. Laminated stalagmites are predominantly found where precipitation has a definite wet or dry season, and no laminated stalagmites have been reported for sites with no precipitation seasonality. Laminated stalagmites are rare in semi-arid climates, and we interpret this to be due to insufficient water availability for continuous stalagmite deposition. Laminated stalagmites from environments with very high rainfall (where rainfall was more than double evaporation) are also rare, which we interpret as due to the continuous nature of recharge.
What do the characteristics of the annual growth layers reveal?
At a global scale, the database reveals how fast stalagmites grow (or more technically, the rate of vertical extension). The mean annual growth rate is 0.163 millimeters, and the median is 0.093 millimeters. In other words, the “global average stalagmite” will have increased in height by about 1 meter over the last 11,000 years.
Our analysis also revealed that the rate of stalagmite growth in a particular year is similar to that of the previous years. However, there is an interesting property related to the rate of change in growth rate from one year to the next, where high growth rate years tend to be followed by low growth years. This is known as “flickering.” The combination of high autocorrelation over several years, and the short-term flickering between years, appears to be a near-universal phenomena generated by water movement and storage in karstified rocks.
What did you set out to learn through your analysis of multiple studies of annually laminated stalagmites from around the world?
We were particularly interested to understand if there were common properties of laminated stalagmites. Before this analysis, we did not have evidence that they are only found in regions with seasonal precipitation. Nor was it obvious that stalagmite accumulation rate is relatively unchanging over time and that this is a ubiquitous property.
What we have learned is that for an environmental signal to be preserved in stalagmite laminae thickness variations requires a large perturbation, such as wet or dry years associated with the El Niño – Southern Oscillation phenomenon or mega-droughts, which can override the buffering effect of the overlying water reservoir. However, in regions where there is a seasonality of precipitation, the long-term constant growth rate of laminated stalagmites provides an unparalleled capacity for precise chronology building.
Synchrotron radiation micro X‐ray fluorescence map of strontium of a modern annually laminated stalagmite from the Cook Islands.
Synchrotron radiation micro X‐ray fluorescence map of strontium of a modern annually laminated stalagmite from the Cook Islands. The strontium concentration varies from 200 parts per million (dark blue) up to 400 parts per million (light blue) and each dark blue band marks the onset of the wet season. The base of the image is 2.2 milimeters. The map was acquired by Andrea Borsato and Silvia Frisia at the X‐ray fluorescence microscopy (XFM) beamline at the Australian Synchrotron, Victoria, Australia. Credit: Andrea Borsato and Silvia Frisia
What are the most significant similarities and differences you found between growth patterns in different locations?
The biggest difference we observed is between the growth rate and temperature. At a global scale, we observe a strong, positive correlation between annual growth rate and mean annual temperature, and therefore also a negative correlation with latitude.
The biggest similarity is the linearity of growth rate. For the majority of samples, and irrespective of depositional environment, we show that the age-depth relationship is almost exactly linear over the timescale of tens to thousands of years. We interpret this as due to the buffering effect of a well-mixed karst water store that is necessary to ensure a continuous stalagmite deposition. This approximation to linearity can be used for geochronological applications, such improving the age-depth models necessary for attributing precise ages to geochemical proxies such as oxygen and carbon isotopes.
What do you think future studies on annually laminated stalagmites should focus on?
We still have limited understanding on how crystals grow within each lamina, so future studies could investigate the internal structure of the laminae and the crystal growth mechanisms involved.
More research could be conducted in climatically sensitive regions where laminated stalagmites are expected to be ubiquitous (such as Ethiopia), and there should be more focus on 2D elemental mapping techniques, as we believe they will reveal widespread chemical laminae in regions with seasonal rainfall (for example, elemental mapping of trace element composition such as strontium using synchrotron or micro- X-ray fluorescence).
Future studies should also work on continuing to expand the database. In particular, this analysis would not have been possible without the community databases such as the SISAL database of speleothem oxygen and carbon isotope composition, which contained numerous annually laminated stalagmites.
When Hurricane Maria hit Puerto Rico on 20 September 2017, it swept through an already glitched infrastructure that had been battered by Hurricane Irma just 2 weeks before.
Crossing Puerto Rico from southeast to northwest, Maria was, according to a NOAA report, “by far the most destructive hurricane to hit Puerto Rico in modern times.” U.S. Geological Survey (USGS) researcher Lindsay Davis agreed: “It is possibly the most devastating hurricane to hit the island since San Ciriaco in 1899.”
Since meteorologists started keeping detailed weather records for the island in 1956, Puerto Rico has seen 129 tropical cyclones. According to a 2019 study, climate change strengthened precipitation caused by Hurricane Maria and is potentially making extreme rainfall more frequent, as the island is 5 times more likely to be struck by such downpours today than it was decades ago. A hotter atmosphere and higher sea surface temperatures could be among the culprits for stronger storms and hurricanes in the Atlantic Ocean.
“Puerto Rico will be vulnerable to hurricanes like Maria and impacts such as landslides in the future,” said Jonathan Godt, director of the USGS Puerto Rico Landslide Hazard Mitigation Project.
Hurricanes and Landslides
More than 70,000 landslides happened in the wake of Hurricane Maria in Puerto Rico. Besides the intensity of the hurricane-caused rainfall, the composition of the soils in Puerto Rico also might make landslides more common. “Many more elements are at play here, but clay-rich sedimentary rock and shales are not very stable and are quite susceptible to landslides,” Godt said.
Utuado, a municipality in the Cordillera Central region of Puerto Rico, was one of the worst-hit areas when Hurricane Maria struck. In Utuado, “soils are sandy and landslides there tend to be frequent, but thin (so a little amount of material at the surface fails); but they flow rapidly, meaning they move far, which is important from a landslide hazard perspective,” Godt added.
Building Bridges
It took 10 days for regional authorities to assist Utuado, and federal aid did not arrive until 42 days later.
The delay is not unprecedented, according to Francisco Valentin, president of the Corporación de Servicios de Salud Primaria y Desarrollo Socioeconómico El Otoao (COSSAO), a local nonprofit focused on health and well-being. “Some neighborhoods in Utuado did not have drinking water and electricity for 15, 20 years before the hurricane,” Valentin said.
Researchers Katja Brundiers and Patrick Holladay have been involved in resilience projects in Puerto Rico for years. Brundiers, of Arizona State University’s School of Sustainability, and Holladay, an associate professor of tourism management from Troy University in Georgia, analyzed the efficiency of Utuado’s community response in a 2019 study.
Tetuán is one of the small Utuadan neighborhoods slow to receive aid. When a major bridge over an aqueduct was destroyed by Hurricane Maria, Brundiers and Holladay explained, Tetuán was stranded. Inhabitants grew weary of being overlooked by regional and federal aid programs and ultimately rebuilt the bridge themselves.  These communities “are really far out in the Central Mountains, and they’re always the last to be responded to. I had never seen an action so fast and so powerful,” Holladay said.
“Everybody showed up for work and rebuilt the bridge in a matter of days. People told me if they had gone to the government, it would have taken 2 years and half a million dollars.”“COSSAO and the locals put together $5,000 or $6,000, got the concrete and the aqueduct pipes, everybody showed up for work and rebuilt the bridge in a matter of days. People told me if they had gone to the government, it would have taken 2 years and half a million dollars to rebuild it,” Holladay added.
“We started working the next day,” Valentín said. “The destruction was such that everyone in the community put themselves to work. We faced lack of water and food, we dealt with people who were sick and hurt, everything was happening at the same time, and we weren’t prepared to the magnitude of what we experienced, but now we’re more prepared for this sort of disaster.”
Although the community responded quickly, Puerto Rico’s overall recovery has been slow but steady. Nearly 4 years after Hurricane Maria, “you can still see houses covered with government-provided tarpaulins,” said environmental health researcher Pablo Méndez-Lazaro of the University of Puerto Rico–Medical Sciences Campus and a contributor to the 2019 study. “Communities are advancing and championing initiatives involving actors such as the third sector [volunteers and nonprofits], academia, and local governments,” he added.
Tetuán Reborn and Agritourism
One way locals are building resilience is by boosting local agriculture. According to the U.S. Department of Agriculture, Maria diminished 80% of Puerto Rico’s crop value. Plantain, banana, and coffee crops were among the hardest hit, and losses in agriculture yields reached $780 million.
Puerto Rico’s food security was threatened long before Hurricane Maria, however. The island imports more than 80% of its food, despite the resources and capacity for greater food production.
“Puerto Rico produced all sorts of food in the 1940s and 1950s—this is a chance to reinvigorate agriculture here.”“There is local agricultural production, but the food distribution companies have an almost absolute control of the market,” Méndez-Lazaro explained. “While it is not hard for producers to grow food, it is quite hard for them to sell it. Local produce is frequently more expensive than imported food, even with taxes and transportation costs added. Since most of the Puerto Rican economy turned to developing its industrial sector decades ago, there is very little economic incentive from the government to boost local agricultural production and make it more competitive.”
Tetuán Reborn, a COSSAO initiative, aims to strengthen food security in Utuado by fostering agroecological activities. “Puerto Rico produced all sorts of food in the 1940s and 1950s—this is a chance to reinvigorate agriculture here,” Méndez-Lazaro said.
The most ambitious aspect of Tetuán Reborn is probably the 5 hectare coffee farm and roasting facility, Hacienda Rullan, owned and managed by COSSAO. In addition to producing and selling coffee, the project will also feature tourist facilities, a heritage museum, agrotherapy programs, and a café.
An old building made mostly of metal was not torn down by Hurricanes Maria or Irma.
Built mostly of metal more than 50 years ago, Tetuán’s old coffee factory withstood Hurricanes Irma and Maria and is now being rehabilitated as a coffee roasting facility and tourism site. Credit: Patrick Holladay
About 40 farms are engaged in COSSAO-led agroecological activities in Utuado, with 10 to 15 active at the moment, according to Holladay. In addition to strengthening the regional agricultural sector, the broad scope of the programs (including hospitality, health, and agriculture) is aimed at attracting young people to farming. Local youth often leave the land looking for better wages in cities or the mainland United States.
Agritourism initiatives “produce food, wine, and promote agri- and culinary tourism. There are several successful initiatives in the region,” Méndez-Lazaro said.
One of them is Amasar, a family-run business that produces breadfruit flour that can be used to cook pancakes, cakes, biscuits, porridges, and soups. It is part of a project of the U.S. National Institute of Food and Agriculture’s Sustainable Agriculture Research and Education program. Founded by Marisol Villalobos and Jesús Martes in 2016, Amasar buys breadfruit from local farmers working with a permaculture approach to land management. “We buy pana [breadfruit] with a fair price to value the hard work of farmers,” Villalobos said.
The family business has gender balance across its sectors and provides training for farmers on best practices for cultivation, growth, and harvest.
Although the COVID-19 pandemic has affected guided visits to its farms, the scenario is not so grim thanks to Amasar’s quick thinking early in the pandemic. “When we first heard of the pandemic in the beginning of 2020, we increased our production so we could keep selling the flour in the times ahead,” said Villalobos.
During the pandemic, Valentín said, one way to boost local agricultural production has been the selling of cajitas saludables (healthy boxes). COSSAO buys produce from local farmers, sorts the food in boxes, and makes them available to Puerto Ricans, who can buy about 15 kilograms (30 pounds) of fresh food for $30.
Food boxes with local produce feed Tetuán inhabitants and provide income to farmers during the pandemic.
Cajitas saludables (healthy boxes) are a source of income for local farmers in Tetuán. Credit: Francisco Valentín
“The project was already a source of income to farmers before the pandemic, but COVID-19 accelerated the process,” Méndez-Lazaro said.
Besides agriculture, COSSAO’s Valentín said, the local community is working on several other actions to build resilience in Puerto Rico, such as distributed water, energy, and communication systems. “Tetuán will be the economic engine of our community resilience model.”
The climate emergency is bigger than many experts, elected officials, and activists realize. Humanity’s greenhouse gas emissions have overheated the Earth’s atmosphere, unleashing punishing heat waves, hurricanes, and other extreme weather—that much is widely understood. The larger problem is that the overheated atmosphere has in turn overheated the oceans, assuring a catastrophic amount of future sea level rise.
As oceans heat up the water rises in part because warm water expands but also because the warmer waters have initiated major melt of polar ice sheets. As a result, average sea levels around the world are now all but certain to rise by at least 20 to 30 feet. That’s enough to put large parts of many coastal cities, home to hundreds of millions of people, under water.
The key questions are how soon this sea level rise will happen and whether humans can cool the atmosphere and oceans quickly enough to prevent part of this.
If seas rise 20 feet over the next 2,000 years, our children and their descendants may find ways to adapt. But if seas rise 20 feet or more over the next 100 to 200 years—which is our current trajectory—the outlook is grim. In that scenario, there could be two feet of sea level rise by 2040, three feet by 2050, and much more to come.
Two to three feet of sea level rise may not sound like much, but it will transform human societies.Two to three feet of sea level rise may not sound like much, but it will transform human societies the world over. In South Florida, where I live, residents will lose access to fresh water. Sewage treatment plants will fail, large areas will persistently flood, and Miami Beach and other barrier islands will be largely abandoned. In China, India, Egypt, and other countries with major river deltas, two to three feet of sea level rise will force the evacuation of tens of millions of people and the loss of vast agricultural lands.
Attempting to limit sea level rise therefore must become an urgent priority, including for the world leaders US president Joe Biden is inviting to a climate summit on Earth Day, April 22. We must reframe how the climate emergency is understood and what it means to combat it. Certainly, it is essential to meet the Paris Agreement goal of limiting temperature rise to 1.5 to 2 degrees Celsius—but that will not be sufficient.
The solution to rapidly rising sea levels is two-fold: Humans must stop putting more heat trapping gases into the atmosphere, and we must extract much of what we’ve already put up there. Since the Industrial Revolution 250 years ago, the amount of CO2 in the atmosphere has soared due to human activities, principally the burning of carbon-based fossil fuels. To minimize future sea level rise, we need to lower that amount from today’s 417 parts per million towards the 280 ppm that prevailed before industrialization.
Halting heat-trapping emissions requires rapidly moving the economy off fossil fuels to renewable energy as well as ending deforestation, shifting to climate-friendly agriculture, planting soil-building forests, and more. But even if we succeed on this front—and so far, we are falling well short—only the atmosphere would stop getting hotter.
Cooling the oceans will be harder. This requires pulling massive amounts of CO2 from both the atmosphere and the oceans and storing it where it cannot leak.
There are prototypes of such “carbon negative” technologies. Methods like incorporating pulverized basaltic lava into fertilizers can lead to CO2 removal and other approaches must be aggressively developed. It is crucial that both strategies—halting further emissions of CO2 and extracting CO2 that’s already been emitted—be pursued. Doing one cannot be an excuse for not doing the other or we will fail.
Our dilemma is rooted in basic physics. Once CO2 is emitted, it remains in the atmosphere for millennia, trapping heat and warming the planet like a blanket warms a human body. What’s insufficiently appreciated is that most of this warming—over 93 percent—has transferred to the oceans and significantly warmed the upper 2,000 feet. This is accelerating polar ice melt and global sea level rise and will continue to do so for centuries.
And sea level rise is accelerating at a dangerous pace. In 1900, global sea levels were rising 0.6 millimeters a year. After 1930, as ocean warming and water expansion kicked in, the rate of sea level rise doubled and doubled again, reaching 3.1 mm a year by 1990. Since then, as ever-warmer oceans have driven polar ice melt, the rate of sea level rise has quickened further. Today, oceans are rising six mm a year (over two inches a decade), and this pace will continue to dramatically accelerate.
Two inches a decade may seem a trifle but remember: We are just at the beginning of this acceleration. The US National Oceanic and Atmospheric Administration projected in 2017 that global mean sea level could rise five to 8.2 feet by 2100. Four years later, it’s clear that eight feet is in fact a moderate projection. And regional influences—subsidence, changing ocean currents, and redistribution of Earth’s mass as ice melts—will cause some local sea level rise to be 20 to 70 percent higher than global.
Sea level rise of eight feet would be catastrophic….It would put much of New York and Washington, D.C., Shanghai and Bangkok, Lagos, Alexandria, and countless other coastal cities underwater.Sea level rise of eight feet would be catastrophic. Absent extensive and very expensive adaptation measures, it would put much of New York and Washington, D.C., Shanghai and Bangkok, Lagos, Alexandria, and countless other coastal cities underwater. It would submerge South Florida. And building sea walls won’t help in South Florida: The land rests on porous limestone, so rising seas will simply seep under. Even the levee-protected Netherlands and New Orleans will be in deep trouble.
Worse, on current trends, we will be lucky for seas to rise “only” eight feet by 2100. The reason is that the computer models used by NOAA and others do not reflect what we know about how seas have risen in the past. These models assume that sea level rise unfolds gradually, but the geological record shows that in fact it can occur in rapid pulses. Warmer temperatures following the previous ice age caused disintegration of one polar ice sector after another, causing seas to rise in pulses of three to 30 feet per century. Today, accelerating ice melt in Greenland and Antarctica are almost certainly the beginning of a new pulse of rapid sea level rise.
It is urgent that humanity transition to renewable energy, stop burning fossil fuels, and develop and deploy technologies to extract CO2 from the skies and seas. We must also get realistic about adapting to the sea level rise that can no longer be prevented. Rather than building more in low-lying regions and spending public money on coastal defenses that are bound to fail, we should prepare to assist the eventual relocation of people and infrastructure from the most threatened areas (and clean the land before inundation).
Without such measures, there will come a point, sooner than many people realize, when civilization as we know it will greatly weaken or outright collapse. We can only prevent this scenario with serious planning, funding, and effort. Our children, and their children, deserve much better than we are doing now.
For decades, the widths of tree rings have offered a precise window into past regional environmental conditions. The oxygen (δ18O) and carbon (δ13C) isotopic signatures of wood cellulose provide an additional, nuanced environmental fingerprint that records subtle shifts in temperature, precipitation, and drought conditions.
“To reconstruct multimillennial chronologies, samples from living trees, historical timbers, archaeological remains, and subfossil materials have to be combined.”Despite the power of this approach, questions remain as to how tree species, site elevation, tree age, and preservation techniques could affect the stable isotopic values captured in the individual samples.
“To reconstruct multimillennial chronologies, samples from living trees, historical timbers, archaeological remains, and subfossil materials have to be combined,” said Otmar Urban, a scientist at the Global Change Research Institute, Czech Academy of Sciences, and first author of a new study on the value of stable isotopes in individual trees. “It could bring problems, because [this information] is usually unknown.”
To address these uncertainties, the researchers developed a new method to evaluate the variability in the stable isotopic record in individual trees. They leveraged a multimillennial tree ring chronology established in the Czech Republic, consisting of about 4,000 core samples obtained from living oaks and historical timbers of the same species. This database provides a mechanism to reconstruct climate conditions across central Europe over the past 1,500 years. The results of the study were published in the February issue of Dendrochronologia.
Worst Summer Droughts in 2,000 Years
The team obtained 21 cores from living oak trees from seven locations across the Czech regions of western Bohemia and eastern Moravia.
Map of the Czech Republic showing where samples of two species of oak trees were taken for a new study
Researchers studied oak samples (Q. robur and Q. petraea) taken from seven locations across the Czech Republic. Credit: Otmar Urban
The samples consisted of two species of oak, English oak (Quercus robur) and sessile oak (Q. petraea), spanning the natural elevations of each species across central Europe. The researchers grouped samples as low elevation (170–250 meters above sea level) and high elevation (450–495 meters above sea level). In addition, samples were grouped as young (<98 years old) and old (149–198 years old). Finally, the team examined how a common preservation method, polyvinyl acetate, could affect the stable isotopic signatures preserved in the tree samples.
The team found that the stable carbon and oxygen isotopic values recorded in the two oak species were similar despite differences in elevation and age. The team also found that the polyvinyl acetate did not bias the isotopic results when the cellulose is suitably extracted.
Nonpooled samples, like the method detailed in this study, reflect the nuance of the stable isotopic record in individual trees.Unlike pooled samples, which reduce cost and analysis times, nonpooled samples, like the method detailed in this study, reflect the nuance of the stable isotopic record in individual trees. The researchers found that older oak wood samples can be combined to improve stable isotope chronologies for long-term, isotope-based paleoclimatic reconstructions.
“Such multimillennial reconstruction would be impossible without previous confirmation of the suitability to combine separate wood samples,” said Urban. “Our data set can further be used as a part of a larger archive for regional to large-scale dendroclimatic investigation with the possibility to extend the data set into the past when new samples are available.”
To prove this point, the team applied this approach in a subsequent study in Nature Geoscience using 147 samples from living and dead European oaks. They were able to reconstruct hydroclimate conditions in central Europe over the past 2,110 years. In particular, they found that recent summer droughts, between 2015 and 2018, have been the most severe throughout the past 2 millennia.
“The study from Otmar Urban contributes valuable methodological insights in the development of nonpooled chronologies of tree ring stable isotopes. From my personal perspective, by combining carbon and oxygen isotopes from tree rings, it is possible to infer how atmospheric humidity and the vapor pressure deficit have changed in the past,” said Hugo de Boer, an assistant professor of global environmental change at Utrecht University, Netherlands, who did not contribute to the project. “If we can develop better chronologies of trees in terms of carbon and oxygen isotopes, we can also reconstruct changes in photosynthesis and transpiration to better understand how trees responded to more recent climate change during the 20th century.”
Amateur and professional meteorologists have been documenting atmospheric activity for centuries. These records are valuable tools for assessing climate change, but enthusiasm is not always coupled with consistency: Poorly documented changes in observing protocols, instrumentation, and measurement locations frustrate scientists. In a field where a shift in average temperature of only a couple of degrees can cause potentially irreversible ecological changes, inconsistent measurements pose a massive problem.
“For most data, we’re very grateful if we have a latitude, longitude, and elevation. Anything more is a bonus.”“For most data, we’re very grateful if we have a latitude, longitude, and elevation,” said climate scientist Peter Thorne of Maynooth University, Ireland. “Anything more is a bonus.”
Thorne aims to change this through a collaboration led by the World Meteorological Organization (WMO). He and others are planning to build a worldwide network of climate monitoring stations that will produce high-quality, consistent records, hopefully for hundreds of years. Although it would be too expensive to replace all climate-observing stations with such sites, the global reference network will give climatologists a set of consistent measurements that they can compare to nearby, less consistent measurements to gain a reliable, long-term view of the global climate.
Thorne and his collaborators proposed the idea in a 2018 paper published in the International Journal of Climatology, and the World Meteorological Organization’s infrastructure committee endorsed it last fall. The authors hope reference stations will become “proverbial islands of truth which you can then use to understand all the weird and wonderful data issues going on around those stations,” Thorne said.
Cohering Multiple Networks
This project follows on the heels of several smaller networks. The U.S. Climate Reference Network (USCRN) consists of more than 100 monitoring stations in remote locations around the United States that have measured temperature, precipitation, soil moisture, and a number of other climate variables since the early 2000s. Similarly, the Global Climate Observing System Reference Upper-Air Network consists of 30 worldwide sites that measure climate variables above Earth’s surface, and Canada runs several hundred reference climate stations.
These projects are in line with the long-standing mission of the WMO’s Centennial Observing Stations: “sustainable observational standards and best practices that facilitate the generation of high-quality time series data.” Scientists associated with the Centennial Observing Stations have kept meticulously consistent climate records for more than a hundred years. Blue Hill Observatory in Milton, Mass., for example, has used the same mercury barometer since 1888 in an effort to maintain consistency.
Zeke Hausfather of The Breakthrough Institute and the Berkeley Earth project helped conceptualize the new global reference network. He said that Centennial Observing Stations are “a gift to modern scientists,” and newer networks build on their work.
The first sites in the new global monitoring system will be existing sites that have long provided high-quality data, like sites from USCRN and, possibly, Centennial Observing Stations. The sites won’t necessarily all use the same equipment, but they’ll all share the mindset that climate data are most useful when changes to collection methods are infrequent and, when they must occur, are rigorously documented.
Scaling Up
Because most established climate monitoring sites are in the Northern Hemisphere, the new reference network will initially be skewed toward one half of the globe. Thorne hopes the network will become truly global within 5–10 years, however, with at least 180 well-spaced stations around the world.
Roísín Commane, an atmospheric scientist at Columbia University who is not involved in setting up the new network, said that cross-cultural communication can be a challenge when Americans and Europeans set up monitoring systems in developing regions. “How do we make sure that things like this are a source of pride for the local community?” she asked.
Commane said a global monitoring system is well worth developing, however, as it will have “massive potential” to increase our understanding of the climate.
Thorne agrees that the scale of the project makes it challenging but added, “if we don’t push ourselves, if we don’t push the observing system, we won’t realize the benefits of what we could observe.”
Solar climate intervention, also known as solar radiation modification, is an approach intended to mitigate the impacts of climate change by reducing the amount of solar energy that the Earth system traps. It sits alongside three other plausible responses to climate risk: emission cuts and decarbonization, atmospheric carbon dioxide (CO2) removal, and adaptation to a changing climate.
Unlike the other approaches, solar climate intervention (SCI), which comprises various techniques, aims to modify Earth’s radiation budget—the amounts and balance of solar energy that Earth absorbs and reflects—directly. Implementing SCI means either decreasing inbound solar (shortwave) radiation by reflecting it back into space before it is absorbed or increasing the amount of outbound terrestrial (longwave) radiation.
Potential methods of SCI include stratospheric aerosol injection (SAI), marine cloud brightening, cirrus cloud thinning, surface albedo modification, and space-based methods involving, for example, mirrors (Figure 1). At present, the potential efficacy and risks of implementing these approaches to reduce climate change are highly uncertain and likely depend on how they are implemented.
The Geoengineering Modeling Research Consortium (GMRC) was founded to coordinate SCI modeling research and to identify and resolve relevant issues with physical models, especially where existing climate research is unlikely to do so. Here we synthesize 2 years of GMRC meetings and research, and we offer specific recommendations for future model development.
Diagram showing different methods of solar climate intervention
Fig. 1. Some of the most widely discussed forms of solar climate intervention increase the quantity of solar radiation reflected back into space, including surface albedo enhancement, marine cloud brightening (MCB), stratospheric aerosol injection (SAI), and space-based methods. Cirrus cloud thinning (CCT) instead involves the removal of cirrus clouds to increase the amount of terrestrial radiation “lost” from the system. All of these methods would alter fluxes of both longwave (red) and shortwave (yellow) light. Credit: Chelsea Thompson, University of Colorado/CIRES and NOAA Chemical Sciences Laboratory
Current Climate Models and SCI Simulations
SCI research is an engineering application of Earth system science focused on developing the capacity to reduce the magnitude of climate change. SCI strategies are commonly investigated using global climate models (GCMs). However, these models are most frequently used to study the consequences of anthropogenic emissions (e.g., of CO2), and they are not optimized to represent SCI’s deployment and effects.
Thus, critical processes in SCI simulation, including fundamental physical, chemical, and biogeophysical processes that occur at subgrid scales (i.e., finer than the resolution of GCMs), are missing or poorly represented by these models (Figure 2). For example, most models used to study stratospheric aerosol intervention cannot resolve the physics and chemistry of an injected aerosol plume in the stratosphere, or its subsequent interactions with cloud processes.
Diagram showing the size scales of different model types, physical atmospheric phenomena, and observing approaches relevant to SCI
Fig. 2. Modeling solar climate intervention involves challenges at multiple size scales. Light brown bars span the scales explicitly represented by types of models relevant to SCI. (GCM is global climate model; ESM is Earth system model.) Blue bars span scales of distinct sets of physical atmospheric phenomena that pose key challenges for SCI (µ* indicates aerosol chemistry and microphysics). The SCI methods most relevant to each process are noted in black type. Green bars span physical scales that can be directly observed by different approaches. Networks of measuring devices (not shown here) could enable, for example, surface monitoring stations to provide information on larger scales.
Broader uncertainties also come into play: Different climate models estimate different radiative forcings (human-induced changes in the radiative budget), resulting in different simulated climate impacts. These differences persist even when the models use a common, crude representation of the effects of SCI, such as a uniform increase in stratospheric aerosol optical depth (i.e., a quantifiable decrease in the “transparency” of the stratosphere).
Meanwhile, in some cases (e.g., cirrus cloud thinning), significant uncertainties remain concerning even the fundamental physics of relevant processes. Because of these uncertainties, even agreement among multiple models does not imply scientific confidence in the result. Targeted and coordinated modeling efforts are needed to quantify and reduce these uncertainties.
Priorities for SCI Research
Solar climate intervention methods all involve local-scale perturbations that affect the regional and global climate through chains of processes operating at increasingly large spatial scales.The forms of SCI most discussed in the literature include SAI, marine cloud brightening, and cirrus cloud thinning. These methods all involve local-scale perturbations that affect the regional and global climate through chains of processes operating at increasingly large spatial scales. The range of scales is too great to be represented by any single numerical model or accurately characterized by any single observational approach (Figure 2).
Predictions of SCI’s net climate impacts are therefore especially sensitive to the accuracy of parameterizations used to characterize processes and variables that occur at or below the scale of typical GCM grid cells. Furthermore, some of the large-scale climate perturbations that SCI efforts may produce are likely to be outside the range of parameters (such as microphysical parameters controlling the behavior of stratospheric aerosols) for which we have observations, such that we are left extrapolating otherwise reliable parameterizations into unknown conditions.
As an example of this problem, we have no direct, real-world examples of how atmospheric circulation—and therefore weather patterns—would respond to a sustained increase in solar reflection. Thus, our simulations of its effects are extrapolations based on our knowledge of “related” phenomena, such as volcanic eruptions in which particles enter the stratosphere in a large, brief event and then fall out of the stratosphere over a few years without replenishment.
To resolve this mismatch of scales, we need a systematic effort to identify whether relevant, small-scale processes are well understood physically; well represented by high-resolution, local-scale modeling; and accurately parameterized for use in global-scale models.
Imperfect Analogues
More direct observations are needed to better understand and more accurately model small-scale processes that are important for geoengineering.A long history of observational atmospheric science has informed models used to study geoengineering, but more direct observations are needed to better understand and more accurately model small-scale processes that are important for geoengineering yet still poorly constrained.
For example, volcanic eruptions help to constrain simulations of large-scale changes to stratospheric aerosol loading and have been discussed as an analogue for geoengineering. However, volcanic aerosols provide little data suitable for modeling the likely behavior of localized emissions resulting from proposed SAI strategies, such as the deployment of aerosols in aircraft plumes.
Another potential analogue comes from observations of interactions between clouds and aerosol emissions from ships. Although these observations provide some information on how marine clouds are likely to respond to brightening, much about cloud responses remains uncertain because of the complexity of these systems. It is also difficult to isolate the effects of aerosol changes, because aerosol emissions from ships are poorly constrained and because these effects are often closely tied to local meteorology. More fundamental research into the dynamic and thermodynamic responses of marine clouds to aerosol perturbations and into how ambient and added aerosols compete for cloud water is needed.
Simulations of cirrus cloud thinning are even less well constrained, necessitating research to investigate questions concerning what makes an effective ice nucleus—that is, what types of solid particles are most likely to form a coating of ice, seeding the formation of ice crystals. Research is also needed to investigate the degree to which existing cirrus clouds are formed homogeneously (ice crystals forming without solid seed particles) or heterogeneously (ice crystals forming around dust and aerosol particles).
Testing Modeled Processes Against Observations
Existing observational data sets can be used to test model accuracy when representing some key processes relevant to SCI mechanisms while at the same time helping to determine what other key processes remain insufficiently constrained. We need to identify both the capability of existing models to reproduce existing observations and where more observations are needed to address knowledge gaps. We can do this by performing systematic simulations that explore the ranges of conditions covered by current modeling approaches.
Such an effort will also provide insights into both internal variability in Earth system conditions (e.g., natural changes in marine conditions associated with El Niño) and true uncertainty in current modeling approaches. This separation between variability and uncertainty is a complex issue with many unknowns. To efficiently use computer resources, a single group using only one or a few models could perform an initial investigation to establish whether a given approach yields a significant response, following which a larger effort by groups such as the Geoengineering Model Intercomparison Project (GeoMIP) would be warranted.
Bridging the Resolution Gap
To improve parameterizations in global models, a coordinated effort involving multiple research groups is needed to bridge the “resolution gaps” evident in Figure 2. Increasing the resolution of global models is useful but not sufficient. We must identify the resolution at which the simulated behavior of each process converges to a common solution (e.g., droplet-level modeling of a cloud compared to global cloud parameterizations) and whether this common solution agrees with available empirical observations. This effort will help to determine physical situations or phenomena for which resolution-aware approaches (e.g., resolution-dependent scaling factors, nested gridding, plume-in-grid methods, and adaptive mesh refinement) might be sufficient to re-create observations, as opposed to those for which a new parameterization or subgrid representation is necessary.
A priority is to develop approaches that explicitly, but efficiently, represent plume-scale processes, such as an SAI plume or cirrus seeding track, in global-scale models. Results from such efforts will need to be compared with existing observations where available, and new observations designed to address the knowledge gaps most relevant to SCI may be needed to verify these approaches’ behavior.
Responses Across Scales
Each local-scale change will drive larger-scale responses, involving a wide array of feedbacks, many of which are poorly represented in large-scale models.The bottom-up view presented above addresses only half of the problem. Each local-scale change will drive larger-scale responses. There is considerable evidence that regional forcing—whether conventional or SCI specific—leads to a global response through multiple teleconnections in all components of the Earth system. A global response can occur, for example, through atmospheric pathways, such as local heating perturbations exciting atmospheric waves. Changes in ocean circulation or land feedbacks like windblown desert sand or dust storms can also produce effects on a global scale. A key element of this problem is that it involves a wide array of feedbacks, many of which are poorly represented in large-scale models.
Existing observations can be used, to a limited extent, to constrain the scope and scale of simulated teleconnections and feedbacks relevant to SCI, but this approach relies on there being clear natural proxies for SCI forcing. No such proxy exists for the regional and global-scale responses to significant perturbations in clouds imposed in targeted regions. Even for SAI, volcanoes provide an imperfect proxy. Investigations, including simplified forcing simulations, are therefore needed to identify SCI-relevant teleconnections and feedbacks that relate regional forcing to global change.
Connecting Models to the Real World
Even though the GMRC’s focus is on coordinating modeling efforts and improving models rather than on acquiring real-world observations, the two efforts must be connected. In some cases, models may be improved by incorporating better representations of processes that are already well constrained by observations. In others, model uncertainty is dominantly caused by a lack of empirical knowledge of processes.
There is, for example, a clear path to improving representations of stratospheric aerosols and marine cloud brightening, which are reasonably well constrained by observations, through the use of multiscale modeling. On the other hand, a major source of uncertainty in predictions of cirrus thinning is the lack of both data and methods needed to estimate the prevalence of homogeneous nucleation in cirrus cloud formation.
We will also need experiments conducted in the field to study effects of SCI-related perturbations specifically and test model representations of small-scale processes. These experiments must fill scientific knowledge gaps and uncertainties (some of which can already be identified) that will be difficult to resolve without observations of controlled perturbations. Such dedicated SCI experiments could be conducted at small enough scales so as not to incur significant regional or global effects. For example, aerosols could be added to a single cirrus cloud or to marine clouds over an area of, say, several square kilometers or injected into a single stratospheric aerosol plume.
Strategies for Moving Ahead
Model-based projections for the change in rainfall as a result of SCI could include an estimate of the probability of staying within limits that avoid either droughts or monsoons.A goal of GMRC is to identify the observations that are most needed to reduce uncertainty in model predictions of SCI responses across spatial scales. Progress toward this goal will require collaboration with observational researchers to collect these observations and estimate empirical uncertainty for relevant parameterizations. It will also require developing structured modeling or model intercomparison exercises for SCI methods that can quantify empirical uncertainties—using an agreed upon set of metrics—that are important for predicting weather and climate impacts of interest to policymakers. For example, model-based projections for the change in rainfall as a result of SCI could include an estimate of the probability of staying within limits that avoid either droughts or monsoons.
Last, continued SCI research will require focused conversations among researchers modeling climate and climate impacts as well as others in the broader scientific community and with decision-makers in government and civil society. These conversations should identify the potential SCI impacts that most urgently need to be quantified and which SCI scenarios are most realistic.
If we pursue these strategies, researchers and policymakers can make substantial and rapid progress in studying and understanding the SCI methods that might eventually be needed to mitigate the risks of climate change facing humanity in the present and into the future.